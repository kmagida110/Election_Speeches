{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBATE_URL = 'http://www.presidency.ucsb.edu/debates.php'\n",
    "last_fetched_at = None\n",
    "import json\n",
    "import urllib.request, time, re, random, hashlib\n",
    "import bs4\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    \"\"\"Load the url compassionately.\"\"\"\n",
    "    \n",
    "    global last_fetched_at\n",
    "    \n",
    "    url_hash = hashlib.sha1(url.encode()).hexdigest()\n",
    "    filename = 'cache/cache-file-{}'.format(url_hash)\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            result = f.read()\n",
    "            if len(result) > 0:\n",
    "                #print(\"Retrieving from cache:\", url)\n",
    "                return result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #print(\"Loading:\", url)\n",
    "    wait_interval = random.randint(3000,10000)\n",
    "    if last_fetched_at is not None:\n",
    "        now = time.time()\n",
    "        elapsed = now - last_fetched_at\n",
    "        if elapsed < wait_interval:\n",
    "            time.sleep((wait_interval - elapsed)/1000)\n",
    "        \n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    req = urllib.request.Request(url, headers = headers)\n",
    "    last_fetched_at = time.time()\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = str(response.read())\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(result)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debate_processing(soup):\n",
    "    return_list = []\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    for table in tables:\n",
    "        if table['width'] == '700' and table['bgcolor'] == \"#FFFFFF\":\n",
    "            actual_table = table\n",
    "    rows = actual_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        try:\n",
    "            link = row.find('a')['href']\n",
    "            cols.append(link)\n",
    "            return_list.append(cols)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_from_speech(link):\n",
    "    result = fetch(link)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_debate_dict():\n",
    "    result = fetch(DEBATE_URL)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    debate_list = debate_processing(soup)\n",
    "    debate_dict = {}\n",
    "    for debate in debate_list:\n",
    "\n",
    "        if ' ' not in debate[0]:\n",
    "            debate = debate[1:]\n",
    "        debate_id = ' '.join(debate[:2])\n",
    "        try:\n",
    "            debate_datetime = time.strptime(debate[0].replace('th','').replace('st',''),'%B %d, %Y')\n",
    "        except:\n",
    "            debate_datetime = None\n",
    "\n",
    "        debate_dict[debate_id] = {}\n",
    "        debate_dict[debate_id]['link'] = debate[2]\n",
    "        debate_dict[debate_id]['time'] = debate_datetime \n",
    "        \n",
    "        try:\n",
    "            debate_dict[debate_id]['soup'] = get_words_from_speech(debate[2])\n",
    "        except:\n",
    "            debate_dict[debate_id]['soup'] = None\n",
    "        \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_politician_names(debate_dict):\n",
    "    for key in debate_dict.keys():\n",
    "        raw = get_soup_text(key)\n",
    "        raw = raw.replace(\"--\", \". \")\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "\n",
    "        #find candidate names, most commonly repeated first words of sentences, not common words\n",
    "        colon_names = []\n",
    "        period_names = []\n",
    "\n",
    "        #get names from before colons\n",
    "        for sent in sents:\n",
    "            if ':' in sent:\n",
    "                sent = sent.split(':')\n",
    "                possible_name = sent[0] + \":\"\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    colon_names.append(possible_name_no_paren)\n",
    "\n",
    "        fdist1 = FreqDist(colon_names)\n",
    "        fdist1_above_5 = [name[0] for name in fdist1.most_common(15) if name[1]>5]\n",
    "        \n",
    "        #get names before periods\n",
    "        for sent in sents:\n",
    "            if len(nltk.word_tokenize(sent))<5:\n",
    "                possible_name = sent\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    period_names.append(possible_name_no_paren)\n",
    "                    \n",
    "        fdist2 = FreqDist(period_names)\n",
    "        fdist2_above_15 = [name[0] for name in fdist2.most_common(15) if name[1]>15]\n",
    "    \n",
    "        #add names to dict\n",
    "        colon_name_highest_freq = fdist1.most_common(1)[0][1]\n",
    "        if colon_name_highest_freq > 20 :\n",
    "            debate_dict[key]['names'] = fdist1_above_5\n",
    "        else:\n",
    "            debate_dict[key]['names'] = fdist2_above_15\n",
    "            \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup_text(dbt):\n",
    "    raw = debate_dict[dbt]['soup'].get_text()\n",
    "    raw = raw.replace(\"\\\\\", \"\")\n",
    "    raw = raw.replace(\".\", \". \")\n",
    "    raw = raw.replace(\"?\", \"? \")\n",
    "    raw = raw.replace(\"!\", \"! \")\n",
    "    raw = raw.replace(\"  \", \" \")\n",
    "    raw = raw.replace(\"-\", \"- \")\n",
    "    raw = raw.replace(\"â€¦\", \". \")\n",
    "    raw = raw.replace(\"...\", \". \")\n",
    "    return raw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_paren(name):\n",
    "    return_name = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in name:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            return_name += i\n",
    "    return return_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def clean_dirty_name_lookup(names):\n",
    "    \n",
    "    lookup_dict = {}\n",
    "    \n",
    "    for name in names:\n",
    "        clean_name = name.split()[-1].upper().replace('.','').replace(')','').replace(';','').replace(':','')\n",
    "        lookup_dict[name] = clean_name\n",
    "    \n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_election_year(year, dbt):\n",
    "    year = debate_dict[dbt]['time'].tm_year\n",
    "    year_mod = year % 4\n",
    "    if year_mod == 0:\n",
    "        election_year = year\n",
    "    else:\n",
    "        election_year = year + (4 - year_mod)\n",
    "    return election_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_names(debate_dict):\n",
    "    # Add debate year\n",
    "    name_years = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        time = debate_dict[dbt]['time']\n",
    "\n",
    "        # Get election year\n",
    "        if time:\n",
    "            election_year = get_election_year(time.tm_year, dbt)\n",
    "        else:\n",
    "            election_year = 'Uncertain Year'\n",
    "        debate_dict[dbt]['election_year'] = election_year\n",
    "\n",
    "        # Add new set of names from debate to name_years dict\n",
    "        if election_year not in name_years:\n",
    "            name_years[election_year] = {'names':set()}\n",
    "\n",
    "        names = set(debate_dict[dbt][\"names\"])\n",
    "        name_years[election_year]['names'] = name_years[election_year]['names'].union(names)\n",
    "\n",
    "    # Reduce all names in one year to a single name\n",
    "    for year in name_years:\n",
    "        name_years[year]['lookup'] = clean_dirty_name_lookup(name_years[year]['names'])\n",
    "\n",
    "    # Add lookup dictionary to debate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        election_year = debate_dict[dbt]['election_year']\n",
    "        debate_dict[dbt]['lookup'] = name_years[election_year]['lookup']\n",
    "        debate_dict[dbt]['clean_names'] = debate_dict[dbt]['lookup'].values()\n",
    "    \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def attribute_text(debate_dict):\n",
    "    #make year/candidate dictionary for text\n",
    "    cand_text_dict = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        year = debate_dict[dbt]['election_year']\n",
    "        cand_text_dict[year] = {}\n",
    "        for cand in debate_dict[dbt][\"clean_names\"]:\n",
    "            cand_text_dict[year][cand] = {}\n",
    "            cand_text_dict[year][cand]['full_text'] = \"\"\n",
    "    \n",
    "    #fill year/candidate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        #set variables\n",
    "        year = debate_dict[dbt][\"election_year\"]\n",
    "        names = debate_dict[dbt][\"names\"]\n",
    "        if \"write\" in names:\n",
    "            names.remove('write')\n",
    "        \n",
    "        #get debate soup\n",
    "        raw = get_soup_text(dbt)\n",
    "        \n",
    "        #tokenize sents\n",
    "        for name in names:\n",
    "            raw = raw.replace(name, \". \" + name)\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "        \n",
    "        #loop through sents\n",
    "        current_speaker = \"\"\n",
    "        got_first_speaker = False\n",
    "        for sent in sents:\n",
    "            new_speaker = (len([name for name in names if name in sent])>0)\n",
    "            if(new_speaker):\n",
    "                got_first_speaker = True\n",
    "                current_speaker_dirty = [name for name in names if name in sent][0]\n",
    "                current_speaker = debate_dict[dbt][\"lookup\"][current_speaker_dirty]\n",
    "            \n",
    "            if(got_first_speaker):\n",
    "                sent_no_name = sent.replace(current_speaker_dirty, \"\")\n",
    "                cand_text_dict[year][current_speaker]['full_text'] = (cand_text_dict[year][current_speaker]['full_text'] + \" \" + sent_no_name)\n",
    "\n",
    "    return cand_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity_model(cand_text_dict):\n",
    "    dumbWords = stopwords.words('english')\n",
    "    political_positions = ['Governor', 'Senator', 'President']\n",
    "    \n",
    "    \n",
    "    #loop through election years\n",
    "    for year in cand_text_dict.keys():\n",
    "        #loop through candidates\n",
    "        for cand in cand_text_dict[year].keys():\n",
    "            #print(year, cand)\n",
    "        \n",
    "            tokens = nltk.word_tokenize(cand_text_dict[year][cand]['full_text'])\n",
    "            text = nltk.Text(tokens)\n",
    "            fdist_tokens = FreqDist(tokens)\n",
    "            \n",
    "            special_words = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word not in political_positions]\n",
    "            cand_text_dict[year][cand][\"special_words\"] = special_words\n",
    "            \n",
    "            special_words_no_caps = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word[0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_words_no_caps\"] = special_words_no_caps\n",
    "            \n",
    "            if len(text)>0:\n",
    "                #avg word len\n",
    "                sum_len = sum([len(word) for word in text])\n",
    "                cand_text_dict[year][cand][\"avg_word_len\"] = sum_len/len(text)\n",
    "                \n",
    "                #avg word len, no stopwords\n",
    "                text_no_dumbWords = [word for word in text if word not in dumbWords]\n",
    "                sum_len = sum([len(word) for word in text_no_dumbWords])\n",
    "                cand_text_dict[year][cand][\"avg_word_len_no_stopword\"] = sum_len/len(text_no_dumbWords)\n",
    "                \n",
    "                #lex diversity                \n",
    "                cand_text_dict[year][cand][\"lex_diversity_no_stopword\"] = (len(set(text_no_dumbWords)) / len(text_no_dumbWords))\n",
    "            \n",
    "            bgrms = list(bigrams(text))\n",
    "            fdist_bgrms = FreqDist(bgrms)\n",
    "            special_bgrms = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1])]\n",
    "            cand_text_dict[year][cand][\"special_bgrms\"] = special_bgrms\n",
    "            \n",
    "            special_bgrms_no_caps = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps\"] = special_bgrms_no_caps\n",
    "            \n",
    "            special_bgrms_no_caps_stopwords = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()\n",
    "                                              and bgm[0] not in dumbWords and bgm[1] not in dumbWords]\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps_stopwords\"] = special_bgrms_no_caps_stopwords\n",
    "            \n",
    "    return cand_text_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "'''#make a dictionary with debate info\n",
    "debate_dict = get_debate_dict()\n",
    "\n",
    "#find the names of the participants\n",
    "debate_dict = find_politician_names(debate_dict)\n",
    "\n",
    "#clean names and years for comparison within electoral years\n",
    "debate_dict = clean_names(debate_dict)\n",
    "\n",
    "#compile all text by candidate-year\n",
    "cand_text_dict = attribute_text(debate_dict)'''\n",
    "\n",
    "#create a model of text similarity\n",
    "cand_text_dict2 = similarity_model(cand_text_dict)\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['special_bgrms_no_caps', 'avg_word_len', 'special_bgrms', 'special_words_no_caps', 'special_words', 'full_text', 'lex_diversity_no_stopword', 'special_bgrms_no_caps_stopwords', 'avg_word_len_no_stopword'])\n"
     ]
    }
   ],
   "source": [
    "print(cand_text_dict2[2016]['TRUMP'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('think', 73), ('believe', 50), ('years', 42), ('going', 37), ('there', 31), ('country', 28), ('about', 27), ('people', 26), ('president', 25), ('dollars', 22)]\n"
     ]
    }
   ],
   "source": [
    "print(FreqDist(cand_text_dict2[1960]['KENNEDY']['special_words_no_caps']).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2170036418435263\n"
     ]
    }
   ],
   "source": [
    "print(cand_text_dict2[1960]['KENNEDY']['lex_diversity_no_stopword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.104138851802404\n",
      "4.573079145002889\n",
      "4.284582571602681\n",
      "4.414370078740157\n",
      "4.446633154079962\n",
      "4.421238938053097\n",
      "4.5636363636363635\n",
      "4.0596617898536955\n",
      "4.136774880255455\n",
      "4.461883408071749\n",
      "4.236434108527132\n",
      "4.280459770114943\n",
      "4.345758354755784\n",
      "4.2677150435771125\n",
      "4.176753121998079\n",
      "3.9336343115124155\n",
      "4.229874776386405\n",
      "4.396773773903994\n",
      "3.1315789473684212\n",
      "4.086842105263158\n",
      "4.194939620471535\n",
      "3.932107496463932\n",
      "4.475577654284878\n",
      "4.6120959332638165\n",
      "3.8774193548387097\n",
      "4.308727034120735\n",
      "3.949882645312716\n",
      "4.171428571428572\n",
      "4.776061776061776\n",
      "3.9394625176803393\n",
      "4.124902419984387\n",
      "4.129302325581396\n",
      "4.2719836400818\n",
      "4.164976133651551\n",
      "4.371165644171779\n",
      "4.141197592142782\n",
      "4.248967454154965\n",
      "4.2532836516068935\n",
      "4.00862911103875\n",
      "4.533333333333333\n",
      "4.529345372460496\n",
      "4.6440677966101696\n",
      "4.432336584186347\n",
      "4.12488928255093\n",
      "4.246820349761526\n",
      "4.2598208132322535\n",
      "3.8880733944954127\n",
      "3.6653089724194112\n",
      "4.71256038647343\n",
      "4.12039312039312\n",
      "4.191732629727353\n",
      "2.30188679245283\n",
      "4.264834478450968\n",
      "4.427848101265822\n",
      "4.278528855938363\n",
      "4.469883158742949\n",
      "4.220938628158844\n",
      "4.2202291401458165\n",
      "4.390158853157691\n",
      "4.941860465116279\n",
      "3.9453125\n",
      "4.274706867671692\n",
      "4.692059140311368\n",
      "4.4868651488616464\n",
      "5.159090909090909\n",
      "4.322443181818182\n",
      "5.2192\n",
      "4.5830337886412655\n",
      "4.7427967221781655\n",
      "5.327402135231317\n",
      "4.741935483870968\n",
      "4.901565995525727\n",
      "4.8034934497816595\n",
      "4.985835694050992\n",
      "4.437086092715232\n",
      "4.5231251633133\n",
      "4.390184757505773\n",
      "4.62999185004075\n",
      "5.136812411847672\n",
      "4.163206871868289\n",
      "5.288288288288288\n",
      "4.839519359145528\n",
      "4.700892857142857\n",
      "4.961722488038277\n",
      "4.153238546603475\n",
      "4.849051864875047\n",
      "4.45334928229665\n",
      "4.842676090665367\n",
      "4.543838862559242\n",
      "4.620200826934436\n",
      "4.687965921192759\n",
      "4.3352855051244505\n",
      "4.640630182421227\n",
      "4.665089877010407\n",
      "5.1568804576934655\n",
      "5.0607242339832865\n",
      "4.480511182108626\n",
      "3.294\n",
      "4.181590795397699\n",
      "4.4536802030456855\n",
      "4.408612440191388\n",
      "4.251979841612671\n",
      "4.186067827681026\n",
      "3.962913907284768\n",
      "4.748062015503876\n",
      "4.217700258397933\n",
      "4.4491525423728815\n",
      "4.289909638554217\n",
      "4.33881466255567\n",
      "4.200947867298578\n",
      "4.698492462311558\n",
      "4.320983379501385\n",
      "4.590277777777778\n",
      "4.331950207468879\n",
      "4.391910688140556\n",
      "4.808705114254624\n",
      "4.451492537313433\n",
      "4.186137506987143\n",
      "4.03423017326384\n",
      "4.552941176470588\n",
      "4.170886075949367\n",
      "4.489421720733428\n",
      "4.1741689373297\n",
      "4.549268292682926\n",
      "4.334551360129923\n",
      "4.125806451612903\n",
      "4.408622305529522\n",
      "4.5831005586592175\n",
      "4.37160511567754\n",
      "4.2546875\n",
      "4.4677144480212085\n",
      "4.469135802469136\n",
      "4.372531418312388\n",
      "4.491428571428571\n",
      "4.051587301587301\n",
      "4.53656220322887\n",
      "4.528666666666667\n",
      "4.739492426522219\n",
      "4.267391304347826\n",
      "4.4140625\n",
      "4.410425844346549\n",
      "4.51507446422085\n",
      "4.5797718631178705\n",
      "4.248009101251422\n",
      "4.069047619047619\n",
      "3.9845454545454544\n",
      "4.406537576687117\n",
      "4.4941176470588236\n",
      "4.2110317105850825\n",
      "4.358667217403277\n",
      "4.202564102564103\n",
      "4.67945205479452\n",
      "4.490799849793466\n",
      "4.381711409395973\n",
      "3.970414201183432\n",
      "4.276422764227642\n",
      "4.542609667504889\n",
      "4.212660731948565\n",
      "3.629274145170966\n",
      "4.371277029783761\n",
      "4.0194040194040195\n",
      "4.631189948263119\n",
      "4.225613954555887\n",
      "4.398136163603417\n",
      "4.2123184777165745\n",
      "4.342611785608528\n",
      "4.24203821656051\n",
      "4.713872832369942\n",
      "4.349325337331335\n",
      "4.540084388185654\n",
      "4.259101471727343\n",
      "3.9981834695731155\n",
      "4.126062616628654\n",
      "4.558988764044944\n",
      "4.583959899749374\n",
      "4.026564215148189\n",
      "4.1191489361702125\n",
      "4.018213356461405\n",
      "4.247178419915152\n",
      "4.683398572415381\n",
      "4.6377251568508395\n",
      "4.531237948322406\n",
      "4.1713395638629285\n",
      "3.9393691110200737\n",
      "3.4941580756013746\n",
      "4.343782581497185\n",
      "4.156326331216414\n",
      "4.375194280385452\n",
      "4.207058467131172\n",
      "4.25\n",
      "4.073743016759776\n",
      "4.240968975775606\n",
      "4.52342592833199\n",
      "4.3700819257611005\n",
      "4.056666666666667\n",
      "4.78021978021978\n",
      "4.655450669914738\n",
      "4.813090909090909\n",
      "4.4987834549878345\n",
      "4.4150258635074255\n",
      "4.121353558926487\n",
      "4.060876020786934\n",
      "4.505050505050505\n",
      "4.505449591280654\n",
      "4.132528735632184\n",
      "4.731650845103033\n",
      "4.278388278388278\n",
      "4.565839694656488\n",
      "4.5728813559322035\n",
      "3.9698944754810674\n",
      "4.164975399753997\n",
      "4.123392091472129\n",
      "2.3968253968253967\n",
      "4.4726114649681525\n",
      "4.361197110423117\n",
      "4.684305472038485\n",
      "4.106962663975782\n",
      "4.388689930873657\n",
      "4.1732851985559565\n",
      "4.706231454005935\n",
      "4.3373565787513035\n",
      "4.15972778222578\n",
      "3.827708703374778\n",
      "4.848101265822785\n",
      "4.146066051481301\n",
      "4.277163553183378\n",
      "4.311369509043928\n",
      "4.914313068453805\n",
      "3.975880178953511\n",
      "4.1843910806174955\n",
      "4.401812688821752\n",
      "4.3246513061570475\n",
      "4.135782747603834\n",
      "4.3940936863543785\n",
      "4.260540319279574\n",
      "4.587075471698113\n",
      "4.270522388059701\n",
      "4.499632346654354\n",
      "4.291644976574701\n",
      "4.328410586343033\n",
      "3.9972862957937583\n",
      "4.582867783985103\n",
      "4.38184609146465\n",
      "4.3279225689368355\n",
      "4.304757894736842\n",
      "4.187704711514235\n",
      "4.416383883764278\n",
      "4.388636363636364\n",
      "4.039228295819935\n",
      "4.450183122804395\n",
      "3.989145603969608\n",
      "4.36196608040201\n",
      "4.417576266442765\n",
      "4.467991169977925\n",
      "4.545199501246882\n",
      "4.207395993836672\n",
      "4.848780487804878\n",
      "4.3108108108108105\n",
      "4.572072072072072\n",
      "4.087706685837527\n",
      "4.37997256515775\n",
      "4.2073055921672005\n",
      "3.6053435114503816\n",
      "4.430183003258962\n",
      "4.061711747441898\n",
      "4.482949392345366\n",
      "4.059398680029332\n",
      "4.129527991218441\n",
      "3.7984862819299905\n",
      "4.05679012345679\n",
      "4.383177570093458\n",
      "4.083453237410072\n",
      "4.048301886792453\n",
      "4.6474435196195\n",
      "4.387557492132656\n",
      "4.662439024390244\n",
      "5.085027197251646\n",
      "5.021001121419105\n",
      "4.9542259557689015\n",
      "4.359977857735953\n",
      "4.466958972752897\n",
      "4.21875\n",
      "4.547344110854503\n",
      "4.429588138385502\n",
      "4.076439790575916\n",
      "4.513871951219512\n",
      "4.53781512605042\n",
      "4.474495848161329\n",
      "4.184842883548983\n",
      "4.51729212656365\n",
      "4.700686947988224\n",
      "4.436117936117936\n",
      "4.206736353077816\n",
      "4.374642743728168\n",
      "4.169014084507042\n",
      "4.630549285176825\n",
      "3.1822916666666665\n",
      "4.228826151560178\n",
      "5.15438596491228\n",
      "3.9772427440633247\n",
      "3.9125\n",
      "4.287216380741561\n",
      "4.006622516556291\n",
      "4.533957219251337\n",
      "4.286513135370781\n",
      "4.196428571428571\n",
      "4.963636363636364\n",
      "4.0321299638989165\n",
      "4.1666747915955735\n",
      "4.354329827701947\n",
      "4.385045778229909\n",
      "4.317617866004963\n",
      "5.250489236790607\n",
      "3.896551724137931\n",
      "4.811846689895471\n",
      "4.365877631255631\n",
      "4.052380952380952\n",
      "4.420017647800328\n",
      "4.291666666666667\n",
      "4.304292829472685\n",
      "4.093877551020408\n",
      "4.43135838150289\n",
      "4.278402774689848\n",
      "4.205659075224292\n",
      "4.673611111111111\n",
      "4.582976117575015\n",
      "4.057990261177512\n",
      "4.025348542458809\n",
      "4.408985144084482\n",
      "4.32057584269663\n",
      "4.6461538461538465\n",
      "4.445272655943508\n",
      "4.0873362445414845\n",
      "4.465976331360947\n",
      "4.236559139784946\n",
      "3.9764243614931236\n",
      "4.28030303030303\n",
      "4.057511737089202\n"
     ]
    }
   ],
   "source": [
    "for yr in cand_text_dict2.keys():\n",
    "    for cand in cand_text_dict2[yr].keys():\n",
    "        if len(cand_text_dict2[yr][cand]['full_text'])>3:\n",
    "            print(cand_text_dict2[yr][cand]['avg_word_len_no_stopword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lookup_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-82c3f51ffb09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookup_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lookup_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
