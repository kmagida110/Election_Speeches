{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBATE_URL = 'http://www.presidency.ucsb.edu/debates.php'\n",
    "last_fetched_at = None\n",
    "import json\n",
    "import urllib.request, time, re, random, hashlib\n",
    "import bs4\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    \"\"\"Load the url compassionately.\"\"\"\n",
    "    \n",
    "    global last_fetched_at\n",
    "    \n",
    "    url_hash = hashlib.sha1(url.encode()).hexdigest()\n",
    "    filename = 'cache/cache-file-{}'.format(url_hash)\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            result = f.read()\n",
    "            if len(result) > 0:\n",
    "                #print(\"Retrieving from cache:\", url)\n",
    "                return result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #print(\"Loading:\", url)\n",
    "    wait_interval = random.randint(3000,10000)\n",
    "    if last_fetched_at is not None:\n",
    "        now = time.time()\n",
    "        elapsed = now - last_fetched_at\n",
    "        if elapsed < wait_interval:\n",
    "            time.sleep((wait_interval - elapsed)/1000)\n",
    "        \n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    req = urllib.request.Request(url, headers = headers)\n",
    "    last_fetched_at = time.time()\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = str(response.read())\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(result)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debate_processing(soup):\n",
    "    return_list = []\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    for table in tables:\n",
    "        if table['width'] == '700' and table['bgcolor'] == \"#FFFFFF\":\n",
    "            actual_table = table\n",
    "    rows = actual_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        try:\n",
    "            link = row.find('a')['href']\n",
    "            cols.append(link)\n",
    "            return_list.append(cols)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_from_speech(link):\n",
    "    result = fetch(link)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_debate_dict():\n",
    "    result = fetch(DEBATE_URL)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    debate_list = debate_processing(soup)\n",
    "    debate_dict = {}\n",
    "    for debate in debate_list:\n",
    "\n",
    "        if ' ' not in debate[0]:\n",
    "            debate = debate[1:]\n",
    "        debate_id = ' '.join(debate[:2])\n",
    "        try:\n",
    "            debate_datetime = time.strptime(debate[0].replace('th','').replace('st',''),'%B %d, %Y')\n",
    "        except:\n",
    "            debate_datetime = None\n",
    "\n",
    "        debate_dict[debate_id] = {}\n",
    "        debate_dict[debate_id]['link'] = debate[2]\n",
    "        debate_dict[debate_id]['time'] = debate_datetime \n",
    "        \n",
    "        try:\n",
    "            debate_dict[debate_id]['soup'] = get_words_from_speech(debate[2])\n",
    "        except:\n",
    "            debate_dict[debate_id]['soup'] = None\n",
    "        \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_politician_names(debate_dict):\n",
    "    for key in debate_dict.keys():\n",
    "        raw = get_soup_text(key)\n",
    "        raw = raw.replace(\"--\", \". \")\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "\n",
    "        #find candidate names, most commonly repeated first words of sentences, not common words\n",
    "        colon_names = []\n",
    "        period_names = []\n",
    "\n",
    "        #get names from before colons\n",
    "        for sent in sents:\n",
    "            if ':' in sent:\n",
    "                sent = sent.split(':')\n",
    "                possible_name = sent[0] + \":\"\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    colon_names.append(possible_name_no_paren)\n",
    "\n",
    "        fdist1 = FreqDist(colon_names)\n",
    "        fdist1_above_5 = [name[0] for name in fdist1.most_common(15) if name[1]>5]\n",
    "        \n",
    "        #get names before periods\n",
    "        for sent in sents:\n",
    "            if len(nltk.word_tokenize(sent))<5:\n",
    "                possible_name = sent\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    period_names.append(possible_name_no_paren)\n",
    "                    \n",
    "        fdist2 = FreqDist(period_names)\n",
    "        fdist2_above_15 = [name[0] for name in fdist2.most_common(15) if name[1]>15]\n",
    "    \n",
    "        #add names to dict\n",
    "        colon_name_highest_freq = fdist1.most_common(1)[0][1]\n",
    "        if colon_name_highest_freq > 20 :\n",
    "            debate_dict[key]['names'] = fdist1_above_5\n",
    "        else:\n",
    "            debate_dict[key]['names'] = fdist2_above_15\n",
    "            \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup_text(dbt):\n",
    "    raw = debate_dict[dbt]['soup'].get_text()\n",
    "    raw = raw.replace(\"\\\\\", \"\")\n",
    "    raw = raw.replace(\".\", \". \")\n",
    "    raw = raw.replace(\"?\", \"? \")\n",
    "    raw = raw.replace(\"!\", \"! \")\n",
    "    raw = raw.replace(\"  \", \" \")\n",
    "    raw = raw.replace(\"-\", \"- \")\n",
    "    raw = raw.replace(\"â€¦\", \". \")\n",
    "    raw = raw.replace(\"...\", \". \")\n",
    "    return raw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_paren(name):\n",
    "    return_name = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in name:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            return_name += i\n",
    "    return return_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def clean_dirty_name_lookup(names):\n",
    "    \n",
    "    lookup_dict = {}\n",
    "    \n",
    "    for name in names:\n",
    "        clean_name = name.split()[-1].upper().replace('.','').replace(')','').replace(';','').replace(':','')\n",
    "        lookup_dict[name] = clean_name\n",
    "    \n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_election_year(year, dbt):\n",
    "    year = debate_dict[dbt]['time'].tm_year\n",
    "    year_mod = year % 4\n",
    "    if year_mod == 0:\n",
    "        election_year = year\n",
    "    else:\n",
    "        election_year = year + (4 - year_mod)\n",
    "    return election_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_names(debate_dict):\n",
    "    # Add debate year\n",
    "    name_years = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        time = debate_dict[dbt]['time']\n",
    "\n",
    "        # Get election year\n",
    "        if time:\n",
    "            election_year = get_election_year(time.tm_year, dbt)\n",
    "        else:\n",
    "            election_year = 'Uncertain Year'\n",
    "        debate_dict[dbt]['election_year'] = election_year\n",
    "\n",
    "        # Add new set of names from debate to name_years dict\n",
    "        if election_year not in name_years:\n",
    "            name_years[election_year] = {'names':set()}\n",
    "\n",
    "        names = set(debate_dict[dbt][\"names\"])\n",
    "        name_years[election_year]['names'] = name_years[election_year]['names'].union(names)\n",
    "\n",
    "    # Reduce all names in one year to a single name\n",
    "    for year in name_years:\n",
    "        name_years[year]['lookup'] = clean_dirty_name_lookup(name_years[year]['names'])\n",
    "\n",
    "    # Add lookup dictionary to debate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        election_year = debate_dict[dbt]['election_year']\n",
    "        debate_dict[dbt]['lookup'] = name_years[election_year]['lookup']\n",
    "        debate_dict[dbt]['clean_names'] = debate_dict[dbt]['lookup'].values()\n",
    "    \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def attribute_text(debate_dict):\n",
    "    #make year/candidate dictionary for text\n",
    "    cand_text_dict = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        year = debate_dict[dbt]['election_year']\n",
    "        cand_text_dict[year] = {}\n",
    "        for cand in debate_dict[dbt][\"clean_names\"]:\n",
    "            cand_text_dict[year][cand] = {}\n",
    "            cand_text_dict[year][cand]['full_text'] = \"\"\n",
    "    \n",
    "    #fill year/candidate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        #set variables\n",
    "        year = debate_dict[dbt][\"election_year\"]\n",
    "        names = debate_dict[dbt][\"names\"]\n",
    "        if \"write\" in names:\n",
    "            names.remove('write')\n",
    "        \n",
    "        #get debate soup\n",
    "        raw = get_soup_text(dbt)\n",
    "        \n",
    "        #tokenize sents\n",
    "        for name in names:\n",
    "            raw = raw.replace(name, \". \" + name)\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "        \n",
    "        #loop through sents\n",
    "        current_speaker = \"\"\n",
    "        got_first_speaker = False\n",
    "        for sent in sents:\n",
    "            new_speaker = (len([name for name in names if name in sent])>0)\n",
    "            if(new_speaker):\n",
    "                got_first_speaker = True\n",
    "                current_speaker_dirty = [name for name in names if name in sent][0]\n",
    "                current_speaker = debate_dict[dbt][\"lookup\"][current_speaker_dirty]\n",
    "            \n",
    "            if(got_first_speaker):\n",
    "                sent_no_name = sent.replace(current_speaker_dirty, \"\")\n",
    "                cand_text_dict[year][current_speaker]['full_text'] = (cand_text_dict[year][current_speaker]['full_text'] + \" \" + sent_no_name)\n",
    "\n",
    "    return cand_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity_model(cand_text_dict):\n",
    "    dumbWords = stopwords.words('english')\n",
    "    political_positions = ['Governor', 'Senator', 'President']\n",
    "    \n",
    "    \n",
    "    #loop through election years\n",
    "    for year in cand_text_dict.keys():\n",
    "        #loop through candidates\n",
    "        for cand in cand_text_dict[year].keys():\n",
    "            #print(year, cand)\n",
    "        \n",
    "            tokens = nltk.word_tokenize(cand_text_dict[year][cand]['full_text'])\n",
    "            text = nltk.Text(tokens)\n",
    "            fdist_tokens = FreqDist(tokens)\n",
    "            \n",
    "            special_words = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word not in political_positions]\n",
    "            cand_text_dict[year][cand][\"special_words\"] = special_words\n",
    "            \n",
    "            special_words_no_caps = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word[0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_words_no_caps\"] = special_words_no_caps\n",
    "            \n",
    "            if len(text)>0:\n",
    "                #avg word len\n",
    "                sum_len = sum([len(word) for word in text])\n",
    "                cand_text_dict[year][cand][\"avg_word_len\"] = sum_len/len(text)\n",
    "                \n",
    "                #avg word len, no stopwords\n",
    "                text_no_dumbWords = [word for word in text if word not in dumbWords]\n",
    "                sum_len = sum([len(word) for word in text_no_dumbWords])\n",
    "                cand_text_dict[year][cand][\"avg_word_len_no_stopword\"] = sum_len/len(text_no_dumbWords)\n",
    "                \n",
    "                #lex diversity                \n",
    "                cand_text_dict[year][cand][\"lex_diversity_no_stopword\"] = (len(set(text_no_dumbWords)) / len(text_no_dumbWords))\n",
    "            \n",
    "            bgrms = list(bigrams(text))\n",
    "            fdist_bgrms = FreqDist(bgrms)\n",
    "            special_bgrms = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1])]\n",
    "            cand_text_dict[year][cand][\"special_bgrms\"] = special_bgrms\n",
    "            \n",
    "            special_bgrms_no_caps = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps\"] = special_bgrms_no_caps\n",
    "            \n",
    "            special_bgrms_no_caps_stopwords = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()\n",
    "                                              and bgm[0] not in dumbWords and bgm[1] not in dumbWords]\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps_stopwords\"] = special_bgrms_no_caps_stopwords\n",
    "            \n",
    "    return cand_text_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "#main\n",
    "#make a dictionary with debate info\n",
    "debate_dict = get_debate_dict()\n",
    "\n",
    "#find the names of the participants\n",
    "debate_dict = find_politician_names(debate_dict)\n",
    "\n",
    "#clean names and years for comparison within electoral years\n",
    "debate_dict = clean_names(debate_dict)\n",
    "\n",
    "#compile all text by candidate-year\n",
    "cand_text_dict = attribute_text(debate_dict)\n",
    "\n",
    "#create a model of text similarity\n",
    "cand_text_dict2 = similarity_model(cand_text_dict)\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['EDWARDS', 'GIBSON', 'DEAN', 'WRITE', 'BROKAW', 'CLARK', 'LEHRER', 'CHENEY', 'LIEBERMAN', 'KERRY', 'BUSH', 'SCHIEFFER', 'IFILL', 'SHARPTON', 'KUCINICH'])\n"
     ]
    }
   ],
   "source": [
    "print(cand_text_dict2[2004].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('going', 96), ('people', 91), ('there', 82), ('about', 73), ('think', 72), ('world', 53), ('opponent', 48), ('believe', 40), ('health', 37), ('country', 37), ('troops', 36), ('right', 34), ('continue', 33), ('weapons', 30), ('voted', 29), ('wrong', 29), ('protect', 26), ('years', 26), ('taxes', 25), ('doing', 24), ('billion', 24), ('strong', 23), ('percent', 23), ('great', 21), ('talks', 21), ('money', 20), ('decisions', 19), ('place', 19), ('better', 19), ('understand', 19), ('every', 19), ('threat', 18), ('never', 18), ('before', 18), ('million', 18), ('system', 18), ('other', 18), ('terror', 17), ('working', 17), ('citizens', 16), ('called', 16), ('costs', 16), ('freedom', 16), ('being', 16), ('together', 16), ('decision', 16), ('really', 15), ('course', 15), ('destruction', 15), ('intelligence', 15)]\n"
     ]
    }
   ],
   "source": [
    "print(FreqDist(cand_text_dict2[2004]['BUSH']['special_words_no_caps']).most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cand_text_dict2[1960]['KENNEDY']['lex_diversity_no_stopword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for yr in cand_text_dict2.keys():\n",
    "    for cand in cand_text_dict2[yr].keys():\n",
    "        if len(cand_text_dict2[yr][cand]['full_text'])>3:\n",
    "            print(cand_text_dict2[yr][cand]['avg_word_len_no_stopword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
