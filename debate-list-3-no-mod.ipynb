{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBATE_URL = 'http://www.presidency.ucsb.edu/debates.php'\n",
    "Q_DENSITY_CUTOFF = .0014\n",
    "\n",
    "last_fetched_at = None\n",
    "import json\n",
    "import urllib.request, time, re, random, hashlib\n",
    "import bs4\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "from itertools import combinations\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import bigrams\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    \"\"\"Load the url compassionately.\"\"\"\n",
    "    \n",
    "    global last_fetched_at\n",
    "    \n",
    "    url_hash = hashlib.sha1(url.encode()).hexdigest()\n",
    "    filename = 'cache/cache-file-{}'.format(url_hash)\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            result = f.read()\n",
    "            if len(result) > 0:\n",
    "                #print(\"Retrieving from cache:\", url)\n",
    "                return result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #print(\"Loading:\", url)\n",
    "    wait_interval = random.randint(3000,10000)\n",
    "    if last_fetched_at is not None:\n",
    "        now = time.time()\n",
    "        elapsed = now - last_fetched_at\n",
    "        if elapsed < wait_interval:\n",
    "            time.sleep((wait_interval - elapsed)/1000)\n",
    "        \n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    req = urllib.request.Request(url, headers = headers)\n",
    "    last_fetched_at = time.time()\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = str(response.read())\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(result)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debate_processing(soup):\n",
    "    return_list = []\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    for table in tables:\n",
    "        if table['width'] == '700' and table['bgcolor'] == \"#FFFFFF\":\n",
    "            actual_table = table\n",
    "    rows = actual_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        try:\n",
    "            link = row.find('a')['href']\n",
    "            cols.append(link)\n",
    "            return_list.append(cols)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_from_speech(link):\n",
    "    result = fetch(link)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_debate_dict():\n",
    "    result = fetch(DEBATE_URL)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    debate_list = debate_processing(soup)\n",
    "    debate_dict = {}\n",
    "    for debate in debate_list:\n",
    "\n",
    "        if ' ' not in debate[0]:\n",
    "            debate = debate[1:]\n",
    "        debate_id = ' '.join(debate[:2])\n",
    "        try:\n",
    "            debate_datetime = time.strptime(debate[0].replace('th','').replace('st',''),'%B %d, %Y')\n",
    "        except:\n",
    "            debate_datetime = None\n",
    "\n",
    "        debate_dict[debate_id] = {}\n",
    "        debate_dict[debate_id]['link'] = debate[2]\n",
    "        debate_dict[debate_id]['time'] = debate_datetime \n",
    "        \n",
    "        try:\n",
    "            debate_dict[debate_id]['soup'] = get_words_from_speech(debate[2])\n",
    "        except:\n",
    "            debate_dict[debate_id]['soup'] = None\n",
    "        \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_politician_names(debate_dict):\n",
    "    for key in debate_dict.keys():\n",
    "        raw = get_soup_text(debate_dict[key])\n",
    "        raw = raw.replace(\"--\", \". \")\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "\n",
    "        #find candidate names, most commonly repeated first words of sentences, not common words\n",
    "        colon_names = []\n",
    "        period_names = []\n",
    "\n",
    "        #get names from before colons\n",
    "        for sent in sents:\n",
    "            if ':' in sent:\n",
    "                sent = sent.split(':')\n",
    "                possible_name = sent[0] + \":\"\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    colon_names.append(possible_name_no_paren)\n",
    "\n",
    "        fdist1 = FreqDist(colon_names)\n",
    "        fdist1_above_5 = [name[0] for name in fdist1.most_common(15) if name[1]>5]\n",
    "        \n",
    "        #get names before periods\n",
    "        for sent in sents:\n",
    "            if len(nltk.word_tokenize(sent))<5:\n",
    "                possible_name = sent\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    period_names.append(possible_name_no_paren)\n",
    "                    \n",
    "        fdist2 = FreqDist(period_names)\n",
    "        fdist2_above_15 = [name[0] for name in fdist2.most_common(15) if name[1]>15]\n",
    "    \n",
    "        #add names to dict\n",
    "        colon_name_highest_freq = fdist1.most_common(1)[0][1]\n",
    "        if colon_name_highest_freq > 20 :\n",
    "            debate_dict[key]['names'] = fdist1_above_5\n",
    "        else:\n",
    "            debate_dict[key]['names'] = fdist2_above_15\n",
    "            \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup_text(dbt):\n",
    "    raw = dbt['soup'].get_text()\n",
    "    raw = raw.replace(\"\\\\\", \"\")\n",
    "    raw = raw.replace(\".\", \". \")\n",
    "    raw = raw.replace(\"?\", \"? \")\n",
    "    raw = raw.replace(\"!\", \"! \")\n",
    "    raw = raw.replace(\"  \", \" \")\n",
    "    raw = raw.replace(\"-\", \"- \")\n",
    "    raw = raw.replace(\"â€¦\", \". \")\n",
    "    raw = raw.replace(\"...\", \". \")\n",
    "    return raw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_paren(name):\n",
    "    return_name = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in name:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            return_name += i\n",
    "    return return_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def clean_dirty_name_lookup(names):\n",
    "    \n",
    "    lookup_dict = {}\n",
    "    \n",
    "    for name in names:\n",
    "        clean_name = name.split()[-1].upper().replace('.','').replace(')','').replace(';','').replace(':','')\n",
    "        lookup_dict[name] = clean_name\n",
    "    \n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_election_year(year, dbt):\n",
    "    year = dbt['time'].tm_year\n",
    "    year_mod = year % 4\n",
    "    if year_mod == 0:\n",
    "        election_year = year\n",
    "    else:\n",
    "        election_year = year + (4 - year_mod)\n",
    "    return election_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_names(debate_dict):\n",
    "    # Add debate year\n",
    "    name_years = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        time = debate_dict[dbt]['time']\n",
    "\n",
    "        # Get election year\n",
    "        if time:\n",
    "            election_year = get_election_year(time.tm_year, debate_dict[dbt])\n",
    "        else:\n",
    "            election_year = 'Uncertain Year'\n",
    "        debate_dict[dbt]['election_year'] = election_year\n",
    "\n",
    "        # Add new set of names from debate to name_years dict\n",
    "        if election_year not in name_years:\n",
    "            name_years[election_year] = {'names':set()}\n",
    "\n",
    "        names = set(debate_dict[dbt][\"names\"])\n",
    "        name_years[election_year]['names'] = name_years[election_year]['names'].union(names)\n",
    "\n",
    "    # Reduce all names in one year to a single name\n",
    "    for year in name_years:\n",
    "        name_years[year]['lookup'] = clean_dirty_name_lookup(name_years[year]['names'])\n",
    "\n",
    "    # Add lookup dictionary to debate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        election_year = debate_dict[dbt]['election_year']\n",
    "        debate_dict[dbt]['lookup'] = name_years[election_year]['lookup']\n",
    "        debate_dict[dbt]['clean_names'] = debate_dict[dbt]['lookup'].values()\n",
    "    \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def attribute_text(debate_dict):\n",
    "    #make year/candidate dictionary for text\n",
    "    cand_text_dict = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        year = debate_dict[dbt]['election_year']\n",
    "        cand_text_dict[year] = {}\n",
    "        for cand in debate_dict[dbt][\"clean_names\"]:\n",
    "            cand_text_dict[year][cand] = {}\n",
    "            cand_text_dict[year][cand]['full_text'] = \"\"\n",
    "    \n",
    "    #fill year/candidate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        #set variables\n",
    "        year = debate_dict[dbt][\"election_year\"]\n",
    "        names = debate_dict[dbt][\"names\"]\n",
    "        if \"write\" in names:\n",
    "            names.remove('write')\n",
    "        \n",
    "        #get debate soup\n",
    "        raw = get_soup_text(debate_dict[dbt])\n",
    "        \n",
    "        #tokenize sents\n",
    "        for name in names:\n",
    "            raw = raw.replace(name, \". \" + name)\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "        \n",
    "        #loop through sents\n",
    "        current_speaker = \"\"\n",
    "        got_first_speaker = False\n",
    "        for sent in sents:\n",
    "            new_speaker = (len([name for name in names if name in sent])>0)\n",
    "            if(new_speaker):\n",
    "                got_first_speaker = True\n",
    "                current_speaker_dirty = [name for name in names if name in sent][0]\n",
    "                current_speaker = debate_dict[dbt][\"lookup\"][current_speaker_dirty]\n",
    "            \n",
    "            if(got_first_speaker):\n",
    "                sent_no_name = sent.replace(current_speaker_dirty, \"\")\n",
    "                cand_text_dict[year][current_speaker]['full_text'] = (cand_text_dict[year][current_speaker]['full_text'] + \" \" + sent_no_name)\n",
    "\n",
    "    return cand_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity_model(cand_text_dict):\n",
    "    dumbWords = stopwords.words('english')\n",
    "    political_positions = ['Governor', 'Senator', 'President']\n",
    "    \n",
    "    \n",
    "    #loop through election years\n",
    "    for year in cand_text_dict.keys():\n",
    "        #loop through candidates\n",
    "        for cand in cand_text_dict[year].keys():\n",
    "            #print(year, cand)\n",
    "        \n",
    "            tokens = nltk.word_tokenize(cand_text_dict[year][cand]['full_text'])\n",
    "            text = nltk.Text(tokens)\n",
    "            fdist_tokens = FreqDist(tokens)\n",
    "            \n",
    "            special_words = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word not in political_positions]\n",
    "            cand_text_dict[year][cand][\"special_words\"] = special_words\n",
    "            \n",
    "            special_words_no_caps = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word[0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_words_no_caps\"] = special_words_no_caps\n",
    "            \n",
    "            if len(text)>0:\n",
    "                #avg word len\n",
    "                sum_len = sum([len(word) for word in text])\n",
    "                cand_text_dict[year][cand][\"avg_word_len\"] = sum_len/len(text)\n",
    "                \n",
    "                #avg word len, no stopwords\n",
    "                text_no_dumbWords = [word for word in text if word not in dumbWords]\n",
    "                sum_len = sum([len(word) for word in text_no_dumbWords])\n",
    "                cand_text_dict[year][cand][\"avg_word_len_no_stopword\"] = sum_len/len(text_no_dumbWords)\n",
    "                \n",
    "                #lex diversity                \n",
    "                cand_text_dict[year][cand][\"lex_diversity_no_stopword\"] = (len(set(text_no_dumbWords)) / len(text_no_dumbWords))\n",
    "            \n",
    "            bgrms = list(bigrams(text))\n",
    "            fdist_bgrms = FreqDist(bgrms)\n",
    "            special_bgrms = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1])]\n",
    "            cand_text_dict[year][cand][\"special_bgrms\"] = special_bgrms\n",
    "            \n",
    "            special_bgrms_no_caps = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps\"] = special_bgrms_no_caps\n",
    "            \n",
    "            special_bgrms_no_caps_stopwords = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()\n",
    "                                              and bgm[0] not in dumbWords and bgm[1] not in dumbWords]\n",
    "            #replace this with compound words\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps_stopwords\"] = special_bgrms_no_caps_stopwords\n",
    "            \n",
    "    return cand_text_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "debate_dict = get_debate_dict()\n",
    "\n",
    "#find the names of the participants\n",
    "debate_dict = find_politician_names(debate_dict)\n",
    "\n",
    "#clean names and years for comparison within electoral years\n",
    "debate_dict = clean_names(debate_dict)\n",
    "\n",
    "#compile all text by candidate-year\n",
    "cand_text_dict = attribute_text(debate_dict)\n",
    "    \n",
    "#identify and remove moderators from attributed text dict\n",
    "cand_text_dict = remove_non_politicians(cand_text_dict)\n",
    "\n",
    "#remove speakers who don't talk enough for this to be cool/useful\n",
    "cand_text_dict = remove_losers(cand_text_dict)\n",
    "\n",
    "#create a model of text similarity\n",
    "#cand_text_dict = similarity_model(cand_text_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_non_politicians(cand_text_dict):\n",
    "    for year in cand_text_dict.keys():\n",
    "        for cand in cand_text_dict[year].keys():\n",
    "            cand_text_dict[year][cand]['q_density'] = 0\n",
    "            text = cand_text_dict[year][cand]['full_text']\n",
    "            num_chars = len(list(text))\n",
    "            num_q_marks = text.count('?')\n",
    "            if num_chars > 0:\n",
    "                cand_text_dict[year][cand]['q_density'] = num_q_marks/num_chars\n",
    "\n",
    "    cand_text_dict_no_mod = copy.deepcopy(cand_text_dict)\n",
    "    #remove liekly moderators\n",
    "    for year in cand_text_dict.keys():\n",
    "        for cand in cand_text_dict[year].keys():\n",
    "            if ((cand == 'MALE' and 'MALE' in cand_text_dict_no_mod[year]) or \n",
    "                (cand == 'UNKNOWN' and 'UNKNOWN' in cand_text_dict_no_mod[year]) or \n",
    "                (cand == 'WRITE' and 'WRITE' in cand_text_dict_no_mod[year])):\n",
    "                del cand_text_dict_no_mod[year][cand]\n",
    "    \n",
    "            if (cand_text_dict[year][cand]['q_density'] > Q_DENSITY_CUTOFF):\n",
    "                if (cand in cand_text_dict_no_mod[year]):\n",
    "                    del cand_text_dict_no_mod[year][cand]\n",
    "\n",
    "        \n",
    "    return cand_text_dict_no_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_losers():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cand_text_dict_no_mod = remove_non_politicians(cand_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['WEBB', 'CLINTON', 'CHAFEE', 'WALKER', 'BUSH', 'HUCKABEE', 'PATAKI', 'CRUZ', 'CHRISTIE', 'FIORINA', 'CARSON', 'PAUL', 'EPPERSON', 'GILMORE', 'GRAHAM', 'SANTORUM', 'TRUMP', \"O'MALLEY\", 'RUBIO', 'JINDAL', 'KASICH', 'SANDERS'])\n"
     ]
    }
   ],
   "source": [
    "print(cand_text_dict_no_mod[2016].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['WEBB', 'WOODRUFF', 'CLINTON', 'CHAFEE', 'QUINTANILLA', 'MACCALLUM', 'BARTIROMO', 'SMITH', 'COONEY', 'WALKER', 'GARRETT', 'HARWOOD', 'RADDATZ', 'TODD', 'MITCHELL', 'TRUMP', 'MALE', 'BUSH', 'HUCKABEE', 'EPPERSON', 'PATAKI', 'BAIER', 'HEMMER', 'CRUZ', 'UNKNOWN', 'MADDOW', 'CAVUTO', 'HEWITT', 'SEIB', 'MCELVEEN', 'CARSON', 'TAPPER', 'CORDES', 'COOPER', 'HOLT', 'JINDAL', 'STRASSEL', 'QUICK', 'WRITE', 'GILMORE', 'GRAHAM', 'SANTORUM', 'LOPEZ', 'FIORINA', 'IFILL', 'BASH', 'CHRISTIE', \"O'MALLEY\", 'RUBIO', 'WALLACE', 'HAM', 'BAKER', 'BLITZER', 'DICKERSON', 'PAUL', 'REGAN', 'KASICH', 'SANDERS', 'MUIR'])\n"
     ]
    }
   ],
   "source": [
    "print(cand_text_dict[2016].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn_direct_compare(debate_dict,var_list,distance_method,weights=None):\n",
    "    \n",
    "    # locations of id and text in score dataframe\n",
    "    cand_id_loc = 0\n",
    "    full_text_loc = 1\n",
    "    dist_dict = {}\n",
    "    cand_scores = []\n",
    "    for year in debate_dict:\n",
    "        # Build each row of dataframe\n",
    "        for candidate in debate_dict[year]:\n",
    "            cand_year_dict = debate_dict[year][candidate]\n",
    "            cand_id = candidate + '_' + str(year)\n",
    "            var_score_list = [cand_year_dict[x] for x in cand_year_dict if x in var_list]\n",
    "            cand_scores.append([cand_id,cand_year_dict['full_text']] + var_score_list)\n",
    "            \n",
    "    # build normalized dataframe with name and text as first columns, var_list as col keys\n",
    "    cand_df = normalize_scores(cand_scores,var_list)\n",
    "    \n",
    "\n",
    "    \n",
    "    tfidf_freq = get_tfidf_vectors(cand_df['full_text'])\n",
    "    feature_names = tfidf_freq.columns\n",
    "    return df\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    # Loop over all combinations and calculate distances\n",
    "    for i in range(num_rows-1):\n",
    "        for j in range(i+1,num_rows):\n",
    "            \n",
    "            # default to equal weights\n",
    "            if weights == None:\n",
    "                weights = [1]*(len(cand1_var_list) + 1)\n",
    "            \n",
    "            df_row = df.ix[i]\n",
    "            weighted1_vect = \n",
    "                \n",
    "            \n",
    "\n",
    "            # weight variables\n",
    "    \n",
    "            # fill both sides of dictionary with relative distances\n",
    "            fill_dist_dict(dist_dict,cand1_name,cand2_name,var_distance,tfdif_dist)         \n",
    "    \n",
    "    return dist_dict              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_score(row1,row2,weights,var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_dist_dict(dist_dict,name1,name2,var_dist,tfdif_dist):\n",
    "    if name1 not in dist_dict:\n",
    "        dist_dict[name1] = {}\n",
    "    if name2 not in dist_dict:\n",
    "        dist_dict[name2] = {}\n",
    "    \n",
    "    cur_dict = dist_dict[name1]\n",
    "    cur_dict[name2] = {'gen_distance':var_dist,'tfdif_dist':tfdif_dist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_row_data_w_tfidf(df,row_num,var_list,word_features,weights):\n",
    "    # returns canidates name, a tfdif matrix and a list of other scores in the matrix\n",
    "    \n",
    "    df_row = df.ix[row_num]\n",
    "    name = df_row['cand_name']\n",
    "    scores = np.array(df_row[var_list])*weights[:-1]\n",
    "    tfidf = 0\n",
    "    return name, tfidf, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_scores(list_of_scores,var_list):\n",
    "    \n",
    "    df = pd.DataFrame(list_of_scores)\n",
    "    \n",
    "    # Rename columns\n",
    "    col_dict = {0:'cand_name',1:'full_text'}\n",
    "    for i, var in enumerate(var_list):\n",
    "        col_dict[i+2] = var\n",
    "    df = df.rename(columns=col_dict)\n",
    "    df.fillna(0,inplace=True) # none existant should be 0\n",
    "    \n",
    "    \n",
    "    df[var_list] = normalize(df[var_list],'l1',axis=0)\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(list_of_texts):\n",
    "    # Used basis of code from hw 1\n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\",\n",
    "                                 tokenizer = None,\n",
    "                                 preprocessor = None,\n",
    "                                 stop_words = stopwords.words('english'),\n",
    "                                 lowercase= True,\n",
    "                                 max_features = 1000,\n",
    "                                 smooth_idf = True)\n",
    "    compressed_vectors = vectorizer.fit_transform(list_of_texts)\n",
    "    df = pd.DataFrame(compressed_vectors.toarray())\n",
    "    df.columns = vectorizer.get_feature_names()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cand = knn_direct_compare(cand_text_dict,['lex_diversity_no_stopword','avg_word_len_no_stopword'],cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cand.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cosine(np.array(cand1_var_list),np.array(cand2_var_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "var_list = ['lex_diversity_no_stopword','avg_word_len_no_stopword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cand.ix[0]['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_freq = pd.DataFrame(frequencies)\n",
    "df_freq.columns = feature_names\n",
    "df_freq.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
