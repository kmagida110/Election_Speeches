{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEBATE_URL = 'http://www.presidency.ucsb.edu/debates.php'\n",
    "last_fetched_at = None\n",
    "import json\n",
    "import urllib.request, time, re, random, hashlib\n",
    "import bs4\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "from itertools import combinations\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import bigrams\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import time\n",
    "SAVE_FILE = 'text_dict.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    \"\"\"Load the url compassionately.\"\"\"\n",
    "    \n",
    "    global last_fetched_at\n",
    "    \n",
    "    url_hash = hashlib.sha1(url.encode()).hexdigest()\n",
    "    filename = 'cache/cache-file-{}'.format(url_hash)\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            result = f.read()\n",
    "            if len(result) > 0:\n",
    "                #print(\"Retrieving from cache:\", url)\n",
    "                return result\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #print(\"Loading:\", url)\n",
    "    wait_interval = random.randint(3000,10000)\n",
    "    if last_fetched_at is not None:\n",
    "        now = time.time()\n",
    "        elapsed = now - last_fetched_at\n",
    "        if elapsed < wait_interval:\n",
    "            time.sleep((wait_interval - elapsed)/1000)\n",
    "        \n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    req = urllib.request.Request(url, headers = headers)\n",
    "    last_fetched_at = time.time()\n",
    "    with urllib.request.urlopen(req) as response:\n",
    "        result = str(response.read())\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(result)\n",
    "    \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debate_processing(soup):\n",
    "    return_list = []\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    for table in tables:\n",
    "        if table['width'] == '700' and table['bgcolor'] == \"#FFFFFF\":\n",
    "            actual_table = table\n",
    "    rows = actual_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        try:\n",
    "            link = row.find('a')['href']\n",
    "            cols.append(link)\n",
    "            return_list.append(cols)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_words_from_speech(link):\n",
    "    result = fetch(link)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_debate_dict():\n",
    "    result = fetch(DEBATE_URL)\n",
    "    soup = bs4.BeautifulSoup(result,'lxml')\n",
    "    debate_list = debate_processing(soup)\n",
    "    debate_dict = {}\n",
    "    for debate in debate_list:\n",
    "\n",
    "        if ' ' not in debate[0]:\n",
    "            debate = debate[1:]\n",
    "        debate_id = ' '.join(debate[:2])\n",
    "        try:\n",
    "            debate_datetime = time.strptime(debate[0].replace('th','').replace('st',''),'%B %d, %Y')\n",
    "        except:\n",
    "            debate_datetime = None\n",
    "\n",
    "        debate_dict[debate_id] = {}\n",
    "        debate_dict[debate_id]['link'] = debate[2]\n",
    "        debate_dict[debate_id]['time'] = debate_datetime \n",
    "        \n",
    "        try:\n",
    "            debate_dict[debate_id]['soup'] = get_words_from_speech(debate[2])\n",
    "        except:\n",
    "            debate_dict[debate_id]['soup'] = None\n",
    "        \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_politician_names(debate_dict):\n",
    "    for key in debate_dict.keys():\n",
    "        raw = get_soup_text(debate_dict[key])\n",
    "        raw = raw.replace(\"--\", \". \")\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "\n",
    "        #find candidate names, most commonly repeated first words of sentences, not common words\n",
    "        colon_names = []\n",
    "        period_names = []\n",
    "\n",
    "        #get names from before colons\n",
    "        for sent in sents:\n",
    "            if ':' in sent:\n",
    "                sent = sent.split(':')\n",
    "                possible_name = sent[0] + \":\"\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    colon_names.append(possible_name_no_paren)\n",
    "\n",
    "        fdist1 = FreqDist(colon_names)\n",
    "        fdist1_above_5 = [name[0] for name in fdist1.most_common(15) if name[1]>5]\n",
    "        \n",
    "        #get names before periods\n",
    "        for sent in sents:\n",
    "            if len(nltk.word_tokenize(sent))<5:\n",
    "                possible_name = sent\n",
    "                possible_name_no_paren = remove_paren(possible_name).strip()\n",
    "                if (len(possible_name_no_paren)<25) & (len(possible_name_no_paren)>2):\n",
    "                    period_names.append(possible_name_no_paren)\n",
    "                    \n",
    "        fdist2 = FreqDist(period_names)\n",
    "        fdist2_above_15 = [name[0] for name in fdist2.most_common(15) if name[1]>15]\n",
    "    \n",
    "        #add names to dict\n",
    "        colon_name_highest_freq = fdist1.most_common(1)[0][1]\n",
    "        if colon_name_highest_freq > 20 :\n",
    "            debate_dict[key]['names'] = fdist1_above_5\n",
    "        else:\n",
    "            debate_dict[key]['names'] = fdist2_above_15\n",
    "            \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup_text(dbt):\n",
    "    raw = dbt['soup'].get_text()\n",
    "    raw = raw.replace(\"\\\\\", \"\")\n",
    "    raw = raw.replace(\".\", \". \")\n",
    "    raw = raw.replace(\"?\", \"? \")\n",
    "    raw = raw.replace(\"!\", \"! \")\n",
    "    raw = raw.replace(\"  \", \" \")\n",
    "    raw = raw.replace(\"-\", \"- \")\n",
    "    raw = raw.replace(\"â€¦\", \". \")\n",
    "    raw = raw.replace(\"...\", \". \")\n",
    "    return raw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_paren(name):\n",
    "    return_name = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in name:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            return_name += i\n",
    "    return return_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def clean_dirty_name_lookup(names):\n",
    "    \n",
    "    lookup_dict = {}\n",
    "    \n",
    "    for name in names:\n",
    "        clean_name = name.split()[-1].upper().replace('.','').replace(')','').replace(';','').replace(':','')\n",
    "        lookup_dict[name] = clean_name\n",
    "    \n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_election_year(year, dbt):\n",
    "    year = dbt['time'].tm_year\n",
    "    year_mod = year % 4\n",
    "    if year_mod == 0:\n",
    "        election_year = year\n",
    "    else:\n",
    "        election_year = year + (4 - year_mod)\n",
    "    return election_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_names(debate_dict):\n",
    "    # Add debate year\n",
    "    name_years = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        time = debate_dict[dbt]['time']\n",
    "\n",
    "        # Get election year\n",
    "        if time:\n",
    "            election_year = get_election_year(time.tm_year, debate_dict[dbt])\n",
    "        else:\n",
    "            election_year = 'Uncertain Year'\n",
    "        debate_dict[dbt]['election_year'] = election_year\n",
    "\n",
    "        # Add new set of names from debate to name_years dict\n",
    "        if election_year not in name_years:\n",
    "            name_years[election_year] = {'names':set()}\n",
    "\n",
    "        names = set(debate_dict[dbt][\"names\"])\n",
    "        name_years[election_year]['names'] = name_years[election_year]['names'].union(names)\n",
    "\n",
    "    # Reduce all names in one year to a single name\n",
    "    for year in name_years:\n",
    "        name_years[year]['lookup'] = clean_dirty_name_lookup(name_years[year]['names'])\n",
    "\n",
    "    # Add lookup dictionary to debate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        election_year = debate_dict[dbt]['election_year']\n",
    "        debate_dict[dbt]['lookup'] = name_years[election_year]['lookup']\n",
    "        debate_dict[dbt]['clean_names'] = debate_dict[dbt]['lookup'].values()\n",
    "    \n",
    "    return debate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def attribute_text(debate_dict):\n",
    "    #make year/candidate dictionary for text\n",
    "    cand_text_dict = {}\n",
    "    for dbt in debate_dict.keys():\n",
    "        year = debate_dict[dbt]['election_year']\n",
    "        cand_text_dict[year] = {}\n",
    "        for cand in debate_dict[dbt][\"clean_names\"]:\n",
    "            cand_text_dict[year][cand] = {}\n",
    "            cand_text_dict[year][cand]['full_text'] = \"\"\n",
    "    \n",
    "    #fill year/candidate dictionary\n",
    "    for dbt in debate_dict.keys():\n",
    "        #set variables\n",
    "        year = debate_dict[dbt][\"election_year\"]\n",
    "        names = debate_dict[dbt][\"names\"]\n",
    "        if \"write\" in names:\n",
    "            names.remove('write')\n",
    "        \n",
    "        #get debate soup\n",
    "        raw = get_soup_text(debate_dict[dbt])\n",
    "        \n",
    "        #tokenize sents\n",
    "        for name in names:\n",
    "            raw = raw.replace(name, \". \" + name)\n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        sents = sent_detector.tokenize(raw.strip())\n",
    "        \n",
    "        #loop through sents\n",
    "        current_speaker = \"\"\n",
    "        got_first_speaker = False\n",
    "        for sent in sents:\n",
    "            new_speaker = (len([name for name in names if name in sent])>0)\n",
    "            if(new_speaker):\n",
    "                got_first_speaker = True\n",
    "                current_speaker_dirty = [name for name in names if name in sent][0]\n",
    "                current_speaker = debate_dict[dbt][\"lookup\"][current_speaker_dirty]\n",
    "            \n",
    "            if(got_first_speaker):\n",
    "                sent_no_name = sent.replace(current_speaker_dirty, \"\")\n",
    "                cand_text_dict[year][current_speaker]['full_text'] = (cand_text_dict[year][current_speaker]['full_text'] + \" \" + sent_no_name)\n",
    "\n",
    "    return cand_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity_model(cand_text_dict):\n",
    "    dumbWords = stopwords.words('english')\n",
    "    political_positions = ['Governor', 'Senator', 'President']\n",
    "    \n",
    "    \n",
    "    #loop through election years\n",
    "    for year in cand_text_dict.keys():\n",
    "        #loop through candidates\n",
    "        for cand in cand_text_dict[year].keys():\n",
    "            #print(year, cand)\n",
    "        \n",
    "            tokens = nltk.word_tokenize(cand_text_dict[year][cand]['full_text'])\n",
    "            text = nltk.Text(tokens)\n",
    "            fdist_tokens = FreqDist(tokens)\n",
    "            \n",
    "            special_words = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word not in political_positions]\n",
    "            cand_text_dict[year][cand][\"special_words\"] = special_words\n",
    "            \n",
    "            special_words_no_caps = [word for word in tokens if len(word)>4 and fdist_tokens[word]>=5 \n",
    "                             and wordnet.synsets(word) and word[0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_words_no_caps\"] = special_words_no_caps\n",
    "            \n",
    "            if len(text)>0:\n",
    "                #avg word len\n",
    "                sum_len = sum([len(word) for word in text])\n",
    "                cand_text_dict[year][cand][\"avg_word_len\"] = sum_len/len(text)\n",
    "                \n",
    "                #avg word len, no stopwords\n",
    "                text_no_dumbWords = [word for word in text if word not in dumbWords]\n",
    "                sum_len = sum([len(word) for word in text_no_dumbWords])\n",
    "                cand_text_dict[year][cand][\"avg_word_len_no_stopword\"] = sum_len/len(text_no_dumbWords)\n",
    "                \n",
    "                #lex diversity                \n",
    "                cand_text_dict[year][cand][\"lex_diversity_no_stopword\"] = (len(set(text_no_dumbWords)) / len(text_no_dumbWords))\n",
    "            \n",
    "            bgrms = list(bigrams(text))\n",
    "            fdist_bgrms = FreqDist(bgrms)\n",
    "            special_bgrms = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1])]\n",
    "            cand_text_dict[year][cand][\"special_bgrms\"] = special_bgrms\n",
    "            \n",
    "            special_bgrms_no_caps = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()]\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps\"] = special_bgrms_no_caps\n",
    "            \n",
    "            special_bgrms_no_caps_stopwords = [bgm for bgm in bgrms if fdist_bgrms[bgm]>2 \n",
    "                             and wordnet.synsets(bgm[0]) and wordnet.synsets(bgm[1]) \n",
    "                                     and bgm[0][0].islower() and bgm[1][0].islower()\n",
    "                                              and bgm[0] not in dumbWords and bgm[1] not in dumbWords]\n",
    "            cand_text_dict[year][cand][\"special_bgrms_no_caps_stopwords\"] = special_bgrms_no_caps_stopwords\n",
    "            \n",
    "    return cand_text_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main_function(filename):\n",
    "    debate_dict = get_debate_dict()\n",
    "\n",
    "    #find the names of the participants\n",
    "    debate_dict = find_politician_names(debate_dict)\n",
    "\n",
    "    #clean names and years for comparison within electoral years\n",
    "    debate_dict = clean_names(debate_dict)\n",
    "\n",
    "    #compile all text by candidate-year\n",
    "    cand_text_dict = attribute_text(debate_dict)\n",
    "\n",
    "    #create a model of text similarity\n",
    "    cand_text_dict = similarity_model(cand_text_dict)\n",
    "\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(cand_text_dict, fp)\n",
    "    return cand_text_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cand_text_dict = main_function(SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['WRITE', 'WHITE', 'PRESIDENT', 'MASHEK', 'NEWMAN', 'VANOCUR', 'WALTERS', 'BUSH', 'BOYD', 'MONDALE', 'FERRARO', 'QUARLES'])\n"
     ]
    }
   ],
   "source": [
    "print(cand_text_dict[1984].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('federal', 'government'), 13)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(cand_text_dict[1960]['NIXON']['special_bgrms_no_caps_stopwords']).most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('foreign', 'policy'), 12),\n",
       " (('tax', 'reduction'), 9),\n",
       " (('present', 'time'), 8),\n",
       " (('tax', 'bill'), 7),\n",
       " (('best', 'interests'), 6),\n",
       " (('defense', 'budget'), 6),\n",
       " (('make', 'certain'), 5),\n",
       " (('good', 'job'), 5),\n",
       " (('year', 'ago'), 4),\n",
       " (('mining', 'bill'), 4),\n",
       " (('cruise', 'missiles'), 4),\n",
       " (('million', 'metric'), 4),\n",
       " (('made', 'available'), 4),\n",
       " (('billion', 'tax'), 4),\n",
       " (('strip', 'mining'), 4),\n",
       " (('metric', 'tons'), 4),\n",
       " (('net', 'result'), 4),\n",
       " (('constitutional', 'amendment'), 4),\n",
       " (('million', 'people'), 4),\n",
       " (('tax', 'relief'), 4)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(cand_text_dict[1976]['PRESIDENT']['special_bgrms_no_caps_stopwords']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['special_bgrms_no_caps_stopwords', 'lex_diversity_no_stopword', 'avg_word_len_no_stopword', 'special_bgrms_no_caps', 'special_words', 'avg_word_len', 'special_words_no_caps', 'full_text', 'special_bgrms'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand_text_dict[1984]['BUSH'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn_direct_compare(debate_dict,var_list,distance_method,weights=None):\n",
    "    \n",
    "    # locations of id and text in score dataframe\n",
    "    cand_scores = []\n",
    "    for year in debate_dict:\n",
    "        # Build each row of dataframe\n",
    "        for candidate in debate_dict[year]:\n",
    "            cand_year_dict = debate_dict[year][candidate]\n",
    "            cand_id = candidate + '_' + str(year)\n",
    "            var_score_list = [cand_year_dict[x] for x in cand_year_dict if x in var_list]\n",
    "            cand_scores.append([cand_id,cand_year_dict['full_text']] + var_score_list)\n",
    "            \n",
    "    # build normalized dataframe with name and text as first columns, var_list as col keys\n",
    "    cand_df = normalize_scores(cand_scores,var_list)   \n",
    "\n",
    "    \n",
    "    tfidf_freq = get_tfidf_vectors(cand_df['full_text'])\n",
    "    tf_cols = list(tfidf_freq.columns)\n",
    "    df = pd.concat([cand_df,tfidf_freq],axis=1)\n",
    "\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    dist_dict = {}\n",
    "    # Loop over all combinations and calculate distances\n",
    "    for i in range(num_rows-1):\n",
    "        print(i)\n",
    "        for j in range(i+1,num_rows):\n",
    "            \n",
    "            # default to equal weights\n",
    "            if weights == None:\n",
    "                weights = [1]*(len(var_list) + 1)\n",
    "            \n",
    "            name1 = df.ix[i]['cand_name']\n",
    "            name2 = df.ix[j]['cand_name']\n",
    "            score = calculate_score(df.ix[i],df.ix[j],weights,var_list,tf_cols,distance_method)\n",
    "            \n",
    "\n",
    "            # fill both sides of dictionary with relative distances\n",
    "\n",
    "            if name1 not in dist_dict:\n",
    "                dist_dict[name1] = {}\n",
    "            if name2 not in dist_dict:\n",
    "                dist_dict[name2] = {}\n",
    "            dist_dict[name1][name2] = score\n",
    "            dist_dict[name2][name1] = score\n",
    "\n",
    "\n",
    "    return df,dist_dict        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_score(row1,row2,weights,var_list,tfidf_cols,distance_method):\n",
    "    \n",
    "    tfidf_weight = weights[-1]\n",
    "    var_weights = weights[:-1]\n",
    "    \n",
    "    \n",
    "    # equal weight to all tfidf features\n",
    "    tfidf_weight_list = [tfidf_weight / len(tfidf_cols)]*len(tfidf_cols)\n",
    "    \n",
    "    weights = [tfidf_weight_list,var_weights]\n",
    "    columns = [tfidf_cols,var_list]\n",
    "    r1_weighted = weight_rows(row1,columns,weights)\n",
    "    r2_weighted = weight_rows(row2,columns,weights)\n",
    "\n",
    "    return distance_method(r1_weighted,r2_weighted)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_rows(row,col_list,weight_list):\n",
    "    rv = np.array(row[col_list[0]]) * weight_list[0]\n",
    "    for i in range(1,len(col_list)):\n",
    "        rv = np.hstack([rv,np.array(row[col_list[0]]*weight_list[0])])\n",
    "    \n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_dist_dict(dist_dict,name1,name2,var_dist,tfdif_dist):\n",
    "    if name1 not in dist_dict:\n",
    "        dist_dict[name1] = {}\n",
    "    if name2 not in dist_dict:\n",
    "        dist_dict[name2] = {}\n",
    "    \n",
    "    cur_dict = dist_dict[name1]\n",
    "    cur_dict[name2] = {'gen_distance':var_dist,'tfdif_dist':tfdif_dist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_scores(list_of_scores,var_list):\n",
    "    \n",
    "    df = pd.DataFrame(list_of_scores)\n",
    "    \n",
    "    # Rename columns\n",
    "    col_dict = {0:'cand_name',1:'full_text'}\n",
    "    for i, var in enumerate(var_list):\n",
    "        col_dict[i+2] = var\n",
    "    df = df.rename(columns=col_dict)\n",
    "    df.fillna(0,inplace=True) # none existent should be 0\n",
    "    \n",
    "    \n",
    "    df[var_list] = normalize(df[var_list],'l1',axis=0)\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(list_of_texts):\n",
    "    # Used basis of code from hw 1\n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\",\n",
    "                                 tokenizer = None,\n",
    "                                 preprocessor = None,\n",
    "                                 stop_words = stopwords.words('english'),\n",
    "                                 lowercase= True,\n",
    "                                 max_features = 1000,\n",
    "                                 smooth_idf = True) # Enable smoothing\n",
    "    compressed_vectors = vectorizer.fit_transform(list_of_texts)\n",
    "    df = pd.DataFrame(compressed_vectors.toarray())\n",
    "    df.columns = vectorizer.get_feature_names()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "raw_scores,dist_dict = knn_direct_compare(cand_text_dict,['lex_diversity_no_stopword','avg_word_len_no_stopword'],cosine,[1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_distance_matrix(raw_scores,dist_dict):\n",
    "    names = list(raw_scores['cand_name'])\n",
    "    score_compare = pd.DataFrame(index=names,columns=names)\n",
    "    for name in names:\n",
    "        new_col = []\n",
    "        for compare_name in names:\n",
    "            try:\n",
    "                new_col.append(dist_dict[name][compare_name])\n",
    "            except:\n",
    "                new_col.append(-1)\n",
    "        score_compare[name] = new_col\n",
    "    score_compare.fillna(-1,inplace=True)\n",
    "    \n",
    "    rv = score_compare.merge(raw_scores,left_index=True,right_on='cand_name')\n",
    "    \n",
    "    return rv.set_index('cand_name')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HARWOOD_2016</th>\n",
       "      <th>REGAN_2016</th>\n",
       "      <th>HUCKABEE_2016</th>\n",
       "      <th>MACCALLUM_2016</th>\n",
       "      <th>FIORINA_2016</th>\n",
       "      <th>CLINTON_2016</th>\n",
       "      <th>WRITE_2016</th>\n",
       "      <th>full_text</th>\n",
       "      <th>lex_diversity_no_stopword</th>\n",
       "      <th>avg_word_len_no_stopword</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cand_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HARWOOD_2016</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.487423</td>\n",
       "      <td>0.582325</td>\n",
       "      <td>0.685687</td>\n",
       "      <td>0.667461</td>\n",
       "      <td>0.588589</td>\n",
       "      <td>-1</td>\n",
       "      <td>We're going to pose this question to all can...</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035381</td>\n",
       "      <td>0.034616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.015531</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGAN_2016</th>\n",
       "      <td>0.487423</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.698662</td>\n",
       "      <td>0.309390</td>\n",
       "      <td>0.707876</td>\n",
       "      <td>0.665506</td>\n",
       "      <td>-1</td>\n",
       "      <td>In Tuesday's State of the Union Address, the...</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HUCKABEE_2016</th>\n",
       "      <td>0.582325</td>\n",
       "      <td>0.698662</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.741610</td>\n",
       "      <td>0.341592</td>\n",
       "      <td>0.232728</td>\n",
       "      <td>-1</td>\n",
       "      <td>I wish I saw the country in the same place t...</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.017561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACCALLUM_2016</th>\n",
       "      <td>0.685687</td>\n",
       "      <td>0.309390</td>\n",
       "      <td>0.741610</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.714529</td>\n",
       "      <td>0.733591</td>\n",
       "      <td>-1</td>\n",
       "      <td>So it will come as no surprise that there is...</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076535</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIORINA_2016</th>\n",
       "      <td>0.667461</td>\n",
       "      <td>0.707876</td>\n",
       "      <td>0.341592</td>\n",
       "      <td>0.714529</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.359843</td>\n",
       "      <td>-1</td>\n",
       "      <td>Well, thank you. Good evening. If I may begi...</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.033189</td>\n",
       "      <td>0.083464</td>\n",
       "      <td>0.055735</td>\n",
       "      <td>0.043116</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.022490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLINTON_2016</th>\n",
       "      <td>0.588589</td>\n",
       "      <td>0.665506</td>\n",
       "      <td>0.232728</td>\n",
       "      <td>0.733591</td>\n",
       "      <td>0.359843</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td>Well, I'm happy to be here in New Hampshire ...</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.023757</td>\n",
       "      <td>0.052203</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>0.057036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRITE_2016</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 1010 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                HARWOOD_2016  REGAN_2016  HUCKABEE_2016  MACCALLUM_2016  \\\n",
       "cand_name                                                                 \n",
       "HARWOOD_2016       -1.000000    0.487423       0.582325        0.685687   \n",
       "REGAN_2016          0.487423   -1.000000       0.698662        0.309390   \n",
       "HUCKABEE_2016       0.582325    0.698662      -1.000000        0.741610   \n",
       "MACCALLUM_2016      0.685687    0.309390       0.741610       -1.000000   \n",
       "FIORINA_2016        0.667461    0.707876       0.341592        0.714529   \n",
       "CLINTON_2016        0.588589    0.665506       0.232728        0.733591   \n",
       "WRITE_2016         -1.000000   -1.000000      -1.000000       -1.000000   \n",
       "\n",
       "                FIORINA_2016  CLINTON_2016  WRITE_2016  \\\n",
       "cand_name                                                \n",
       "HARWOOD_2016        0.667461      0.588589          -1   \n",
       "REGAN_2016          0.707876      0.665506          -1   \n",
       "HUCKABEE_2016       0.341592      0.232728          -1   \n",
       "MACCALLUM_2016      0.714529      0.733591          -1   \n",
       "FIORINA_2016       -1.000000      0.359843          -1   \n",
       "CLINTON_2016        0.359843     -1.000000          -1   \n",
       "WRITE_2016         -1.000000     -1.000000          -1   \n",
       "\n",
       "                                                        full_text  \\\n",
       "cand_name                                                           \n",
       "HARWOOD_2016      We're going to pose this question to all can...   \n",
       "REGAN_2016        In Tuesday's State of the Union Address, the...   \n",
       "HUCKABEE_2016     I wish I saw the country in the same place t...   \n",
       "MACCALLUM_2016    So it will come as no surprise that there is...   \n",
       "FIORINA_2016      Well, thank you. Good evening. If I may begi...   \n",
       "CLINTON_2016      Well, I'm happy to be here in New Hampshire ...   \n",
       "WRITE_2016                                                          \n",
       "\n",
       "                lex_diversity_no_stopword  avg_word_len_no_stopword    ...     \\\n",
       "cand_name                                                              ...      \n",
       "HARWOOD_2016                     0.002714                  0.002883    ...      \n",
       "REGAN_2016                       0.003328                  0.002907    ...      \n",
       "HUCKABEE_2016                    0.001824                  0.002818    ...      \n",
       "MACCALLUM_2016                   0.003791                  0.002807    ...      \n",
       "FIORINA_2016                     0.001809                  0.002932    ...      \n",
       "CLINTON_2016                     0.001205                  0.003016    ...      \n",
       "WRITE_2016                       0.000000                  0.000000    ...      \n",
       "\n",
       "                   wrong     wrote       www      yeah      year     years  \\\n",
       "cand_name                                                                    \n",
       "HARWOOD_2016    0.035381  0.034616  0.000000  0.000000  0.000000  0.037513   \n",
       "REGAN_2016      0.000000  0.000000  0.038879  0.000000  0.022723  0.000000   \n",
       "HUCKABEE_2016   0.003794  0.000000  0.000000  0.011801  0.025915  0.066378   \n",
       "MACCALLUM_2016  0.000000  0.000000  0.056597  0.000000  0.033078  0.000000   \n",
       "FIORINA_2016    0.027213  0.000000  0.000000  0.006045  0.033189  0.083464   \n",
       "CLINTON_2016    0.008963  0.002192  0.000000  0.009292  0.011478  0.023757   \n",
       "WRITE_2016      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                     yes       yet      york     young  \n",
       "cand_name                                               \n",
       "HARWOOD_2016    0.010568  0.013081  0.015531  0.000000  \n",
       "REGAN_2016      0.000000  0.000000  0.000000  0.000000  \n",
       "HUCKABEE_2016   0.010200  0.004208  0.009994  0.017561  \n",
       "MACCALLUM_2016  0.000000  0.000000  0.076535  0.000000  \n",
       "FIORINA_2016    0.055735  0.043116  0.005119  0.022490  \n",
       "CLINTON_2016    0.052203  0.006627  0.015737  0.057036  \n",
       "WRITE_2016      0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[7 rows x 1010 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_distance_matrix(raw_scores,dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cand_name</th>\n",
       "      <th>full_text</th>\n",
       "      <th>lex_diversity_no_stopword</th>\n",
       "      <th>avg_word_len_no_stopword</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>...</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>www</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HARWOOD_2016</td>\n",
       "      <td>We're going to pose this question to all can...</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>0.025803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035381</td>\n",
       "      <td>0.034616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037513</td>\n",
       "      <td>0.010568</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.015531</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REGAN_2016</td>\n",
       "      <td>In Tuesday's State of the Union Address, the...</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038879</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HUCKABEE_2016</td>\n",
       "      <td>I wish I saw the country in the same place t...</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.051941</td>\n",
       "      <td>0.024904</td>\n",
       "      <td>0.024198</td>\n",
       "      <td>0.004520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.025915</td>\n",
       "      <td>0.066378</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.009994</td>\n",
       "      <td>0.017561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MACCALLUM_2016</td>\n",
       "      <td>So it will come as no surprise that there is...</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>0.018942</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056597</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076535</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FIORINA_2016</td>\n",
       "      <td>Well, thank you. Good evening. If I may begi...</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.072222</td>\n",
       "      <td>0.008505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.033189</td>\n",
       "      <td>0.083464</td>\n",
       "      <td>0.055735</td>\n",
       "      <td>0.043116</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.022490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CLINTON_2016</td>\n",
       "      <td>Well, I'm happy to be here in New Hampshire ...</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.013146</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.013337</td>\n",
       "      <td>0.024910</td>\n",
       "      <td>0.007869</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009292</td>\n",
       "      <td>0.011478</td>\n",
       "      <td>0.023757</td>\n",
       "      <td>0.052203</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.015737</td>\n",
       "      <td>0.057036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WRITE_2016</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows Ã— 1004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cand_name                                          full_text  \\\n",
       "0    HARWOOD_2016    We're going to pose this question to all can...   \n",
       "1      REGAN_2016    In Tuesday's State of the Union Address, the...   \n",
       "2   HUCKABEE_2016    I wish I saw the country in the same place t...   \n",
       "3  MACCALLUM_2016    So it will come as no surprise that there is...   \n",
       "4    FIORINA_2016    Well, thank you. Good evening. If I may begi...   \n",
       "5    CLINTON_2016    Well, I'm happy to be here in New Hampshire ...   \n",
       "6      WRITE_2016                                                      \n",
       "\n",
       "   lex_diversity_no_stopword  avg_word_len_no_stopword       000        10  \\\n",
       "0                   0.002714                  0.002883  0.011532  0.025803   \n",
       "1                   0.003328                  0.002907  0.013012  0.000000   \n",
       "2                   0.001824                  0.002818  0.051941  0.024904   \n",
       "3                   0.003791                  0.002807  0.018942  0.021192   \n",
       "4                   0.001809                  0.002932  0.072222  0.008505   \n",
       "5                   0.001205                  0.003016  0.013146  0.011439   \n",
       "6                   0.000000                  0.000000  0.000000  0.000000   \n",
       "\n",
       "        100        11        12        15    ...        wrong     wrote  \\\n",
       "0  0.000000  0.014048  0.000000  0.014048    ...     0.035381  0.034616   \n",
       "1  0.000000  0.015851  0.017525  0.000000    ...     0.000000  0.000000   \n",
       "2  0.024198  0.004520  0.000000  0.009039    ...     0.003794  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000    ...     0.000000  0.000000   \n",
       "4  0.000000  0.013892  0.005119  0.013892    ...     0.027213  0.000000   \n",
       "5  0.013337  0.024910  0.007869  0.005338    ...     0.008963  0.002192   \n",
       "6  0.000000  0.000000  0.000000  0.000000    ...     0.000000  0.000000   \n",
       "\n",
       "        www      yeah      year     years       yes       yet      york  \\\n",
       "0  0.000000  0.000000  0.000000  0.037513  0.010568  0.013081  0.015531   \n",
       "1  0.038879  0.000000  0.022723  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.011801  0.025915  0.066378  0.010200  0.004208  0.009994   \n",
       "3  0.056597  0.000000  0.033078  0.000000  0.000000  0.000000  0.076535   \n",
       "4  0.000000  0.006045  0.033189  0.083464  0.055735  0.043116  0.005119   \n",
       "5  0.000000  0.009292  0.011478  0.023757  0.052203  0.006627  0.015737   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      young  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.017561  \n",
       "3  0.000000  \n",
       "4  0.022490  \n",
       "5  0.057036  \n",
       "6  0.000000  \n",
       "\n",
       "[7 rows x 1004 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
